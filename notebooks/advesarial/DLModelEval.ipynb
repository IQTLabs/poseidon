{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/fs4/datasets/pcaps/smallFlows.pcap'\n",
    "modelPath = '/data/fs4/home/bradh/outputs/cpuVersion.pickle'\n",
    "modelType = 'cpu' # 'gpu' or 'cpu'\n",
    "dimIn=257\n",
    "maxPackets=2\n",
    "packetReverse = False\n",
    "packetTimeSteps = 16\n",
    "padOldTimeSteps=True\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=%s' % modelType\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import cPickle\n",
    "import sys\n",
    "import binascii\n",
    "import logging\n",
    "#import ipaddress\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP,\\\n",
    "                          Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_header(line):  # pragma: no cover\n",
    "    ret_dict = {}\n",
    "    h = line.split()\n",
    "    if h[2] == 'IP6':\n",
    "        \"\"\"\n",
    "        Conditional formatting based on ethernet type.\n",
    "        IPv4 format: 0.0.0.0.port\n",
    "        IPv6 format (one of many): 0:0:0:0:0:0.port\n",
    "        \"\"\"\n",
    "        ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "        ret_dict['src_ip'] = h[3].split('.')[0]\n",
    "        ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "        ret_dict['dest_ip'] = h[5].split('.')[0]\n",
    "    else:\n",
    "        if len(h[3].split('.')) > 4:\n",
    "            ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "            ret_dict['src_ip'] = '.'.join(h[3].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['src_ip'] = h[3]\n",
    "            ret_dict['src_port'] = ''\n",
    "        if len(h[5].split('.')) > 4:\n",
    "            ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "            ret_dict['dest_ip'] = '.'.join(h[5].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['dest_ip'] = h[5].split(':')[0]\n",
    "            ret_dict['dest_port'] = ''\n",
    "    return ret_dict\n",
    "\n",
    "\n",
    "def parse_data(line):  # pragma: no cover\n",
    "    ret_str = ''\n",
    "    h, d = line.split(':', 1)\n",
    "    ret_str = d.strip().replace(' ', '')\n",
    "    return ret_str\n",
    "\n",
    "\n",
    "def process_packet(output):  # pragma: no cover\n",
    "    # TODO!! throws away the first packet!\n",
    "    ret_header = {}\n",
    "    ret_dict = {}\n",
    "    ret_data = ''\n",
    "    hasHeader = False\n",
    "    for line in output:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if not line.startswith('0x'):\n",
    "                # header line\n",
    "                if ret_dict and ret_data:\n",
    "                    # about to start new header, finished with hex\n",
    "                    ret_dict['data'] = ret_data\n",
    "                    yield ret_dict\n",
    "                    ret_dict.clear()\n",
    "                    ret_header.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "\n",
    "                # parse next header\n",
    "                try:\n",
    "                    ret_header = parse_header(line)\n",
    "                    ret_dict.update(ret_header)\n",
    "                    hasHeader = True\n",
    "                except:\n",
    "                    ret_header.clear()\n",
    "                    ret_dict.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "\n",
    "            else:\n",
    "                # hex data line\n",
    "                if hasHeader:\n",
    "                    data = parse_data(line)\n",
    "                    ret_data = ret_data + data\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "def is_clean_packet(packet):  # pragma: no cover\n",
    "    \"\"\"\n",
    "    Returns whether or not the parsed packet is valid\n",
    "    or not. Checks that both the src and dest\n",
    "    ports are integers. Checks that src and dest IPs\n",
    "    are valid address formats. Checks that packet data\n",
    "    is hex. Returns True if all tests pass, False otherwise.\n",
    "    \"\"\"\n",
    "    if not packet['src_port'].isdigit(): return False\n",
    "    if not packet['dest_port'].isdigit(): return False\n",
    "\n",
    "    if packet['src_ip'].isalpha(): return False\n",
    "    if packet['dest_ip'].isalpha(): return False\n",
    "\n",
    "    if 'data' in packet:\n",
    "        try:\n",
    "            int(packet['data'], 16)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def order_keys(hexSessionDict):\n",
    "    \"\"\"\n",
    "    Returns list of the hex sessions in (rough) time order.\n",
    "    \"\"\"\n",
    "    orderedKeys = []\n",
    "\n",
    "    for key in sorted(hexSessionDict.keys(), key=lambda key: hexSessionDict[key][1]):\n",
    "        orderedKeys.append(key)\n",
    "\n",
    "    return orderedKeys\n",
    "\n",
    "\n",
    "def read_pcap(path):  # pragma: no cover\n",
    "    print 'starting reading pcap file'\n",
    "    hex_sessions = {} \n",
    "    proc = subprocess.Popen('tcpdump -nn -tttt -xx -r '+path,\n",
    "                            shell=True,\n",
    "                            stdout=subprocess.PIPE)\n",
    "    insert_num = 0  # keeps track of insertion order into dict\n",
    "    for packet in process_packet(proc.stdout):\n",
    "        if not is_clean_packet(packet):\n",
    "            continue\n",
    "        if 'data' in packet:\n",
    "            key = (packet['src_ip']+\":\"+packet['src_port'], packet['dest_ip']+\":\"+packet['dest_port'])\n",
    "            rev_key = (key[1], key[0])\n",
    "            if key in hex_sessions:\n",
    "                hex_sessions[key][0].append(packet['data'])\n",
    "            elif rev_key in hex_sessions:\n",
    "                hex_sessions[rev_key][0].append(packet['data'])\n",
    "            else:\n",
    "                hex_sessions[key] = ([packet['data']], insert_num)\n",
    "                insert_num += 1\n",
    "\n",
    "    print 'finished reading pcap file'\n",
    "    return hex_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeBadSessionizer(hexSessionDict, saveFile=False, dataPath=None, fileName=None):  # pragma: no cover\n",
    "    for ses in hexSessionDict.keys():\n",
    "        paclens = []\n",
    "        for pac in hexSessionDict[ses][0]:\n",
    "            paclens.append(len(pac))\n",
    "        if np.min(paclens)<80:\n",
    "            del hexSessionDict[ses]\n",
    "\n",
    "    if saveFile:\n",
    "        print 'pickling sessions'\n",
    "        pickleFile(hexSessionDict, filePath=dataPath, fileName=fileName)\n",
    "\n",
    "    return hexSessionDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadFile(filePath):  # pragma: no cover\n",
    "    file2open = file(filePath, 'rb')\n",
    "    loadedFile = cPickle.load(file2open)\n",
    "    file2open.close()\n",
    "    return loadedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hexTokenizer():  # pragma: no cover\n",
    "    hexstring = '''0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F,\n",
    "                   10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 1A, 1B,\n",
    "                   1C, 1D, 1E, 1F, 20, 21, 22, 23, 24, 25, 26, 27,\n",
    "                   28, 29, 2A, 2B, 2C, 2D, 2E, 2F, 30, 31, 32, 33,\n",
    "                   34, 35, 36, 37, 38, 39, 3A, 3B, 3C, 3D, 3E, 3F,\n",
    "                   40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 4A, 4B,\n",
    "                   4C, 4D, 4E, 4F, 50, 51, 52, 53, 54, 55, 56, 57,\n",
    "                   58, 59, 5A, 5B, 5C, 5D, 5E, 5F, 60, 61, 62, 63,\n",
    "                   64, 65, 66, 67, 68, 69, 6A, 6B, 6C, 6D, 6E, 6F,\n",
    "                   70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 7A, 7B,\n",
    "                   7C, 7D, 7E, 7F, 80, 81, 82, 83, 84, 85, 86, 87,\n",
    "                   88, 89, 8A, 8B, 8C, 8D, 8E, 8F, 90, 91, 92, 93,\n",
    "                   94, 95, 96, 97, 98, 99, 9A, 9B, 9C, 9D, 9E, 9F,\n",
    "                   A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, AA, AB,\n",
    "                   AC, AD, AE, AF, B0, B1, B2, B3, B4, B5, B6, B7,\n",
    "                   B8, B9, BA, BB, BC, BD, BE, BF, C0, C1, C2, C3,\n",
    "                   C4, C5, C6, C7, C8, C9, CA, CB, CC, CD, CE, CF,\n",
    "                   D0, D1, D2, D3, D4, D5, D6, D7, D8, D9, DA, DB,\n",
    "                   DC, DD, DE, DF, E0, E1, E2, E3, E4, E5, E6, E7,\n",
    "                   E8, E9, EA, EB, EC, ED, EE, EF, F0, F1, F2, F3,\n",
    "                   F4, F5, F6, F7, F8, F9, FA, FB, FC, FD, FE, FF'''.replace('\\t', '')\n",
    "\n",
    "    hexList = [x.strip() for x in hexstring.lower().split(',')]\n",
    "    hexList.append('<EOP>')  # End Of Packet token\n",
    "    hexDict = {}\n",
    "\n",
    "    for key, val in enumerate(hexList):\n",
    "        if len(val) == 1:\n",
    "            val = '0'+val\n",
    "        hexDict[val] = key  #dictionary k=hex, v=int  \n",
    "\n",
    "    return hexDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(index, granular = 'hex'):  # pragma: no cover\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    "\n",
    "\n",
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets=2, packetTimeSteps=100,\n",
    "                      packetReverse=False, charLevel=False, padOldTimeSteps=True):  # pragma: no cover\n",
    "    \n",
    "    sessionCollect = []\n",
    "    packetCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = copy(sessionPackets[:maxPackets])\n",
    "    else:\n",
    "        sessionList = copy(sessionPackets)\n",
    "\n",
    "    for packet in sessionList:\n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "            \n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "            packet = packet[:packetTimeSteps]\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        else:\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "        packetCollect.append(packet)\n",
    "        \n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the\n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession,\n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    \n",
    "    return sessionCollect, packetCollect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(dataPath=dataPath, modelPath=modelPath, \n",
    "            dimIn=dimIn, maxPackets=maxPackets, packetReverse = packetReverse,\n",
    "            packetTimeSteps = packetTimeSteps, padOldTimeSteps=padOldTimeSteps):\n",
    "    \n",
    "    print 'sessionizing pcap file'\n",
    "    hexSessions = read_pcap(dataPath)\n",
    "    hexDict = hexTokenizer()\n",
    "    \n",
    "    print 'loading model'\n",
    "    prediction = loadFile(modelPath)\n",
    "\n",
    "    trainingSessions = []\n",
    "    \n",
    "    print 'predicting pcap file'\n",
    "    for session in hexSessions.keys():\n",
    "        oneHotSes = oneSessionEncoder(hexSessions[session][0],\n",
    "                                      hexDict = hexDict,\n",
    "                                      packetReverse = packetReverse, \n",
    "                                      padOldTimeSteps = padOldTimeSteps, \n",
    "                                      maxPackets = maxPackets, \n",
    "                                      packetTimeSteps = packetTimeSteps)\n",
    "\n",
    "        trainingSessions.append(oneHotSes[0])\n",
    "\n",
    "    sessionsMinibatch = np.asarray(trainingSessions, dtype=theano.config.floatX)\\\n",
    "                                   .reshape((-1, packetTimeSteps, 1, dimIn))\n",
    "\n",
    "    predprobs = prediction(sessionsMinibatch)\n",
    "    predtargets = np.argmax(predprobs,axis=1)\n",
    "\n",
    "    return predprobs, predtargets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs, tars = predict(dataPath=dataPath, modelPath=modelPath, \n",
    "            dimIn=dimIn, maxPackets=maxPackets, packetReverse = packetReverse,\n",
    "            packetTimeSteps = packetTimeSteps, padOldTimeSteps=padOldTimeSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
