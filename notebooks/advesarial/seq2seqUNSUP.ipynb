{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how data are represented at each level (forward, backward, forward with padding on top) needs a little\n",
    "    #experimentation to determine the best representation\n",
    "    #also, is encoding at each layer really the best way? or just feeding the raw through?\n",
    "    \n",
    "#Outside web ips are going to be a problem/messy/noisy. Start by categorizing all outside ips by <OUTSIDE_IP>\n",
    "    #instead of the ip address, or another 4 digit symbol to insert into the hex string.\n",
    "    \n",
    "#to help the models generalize more, for a given source ip address with probability p (say p = 0.1) \n",
    "    #use the token <OTHER_MACHINE>\n",
    "    \n",
    "#should we remove random parts of the header, i.e. checksum\n",
    "\n",
    "#should I take out bias for RNNs?\n",
    "\n",
    "#for the decoder,does the fork encoding need to happen ?\n",
    "    #do we simply cat the hContext with the next words?\n",
    "    \n",
    "#Should the architecture just be encode, context and then prediction???\n",
    "\n",
    "#Input data, should it have character and hex pair encoding as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=cpu'#,optimizer=fast_compile'\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import cPickle\n",
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "#import ipaddress\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP,\\\n",
    "                          Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "\n",
    "module_logger = logging.getLogger(__name__)\n",
    "sys.setrecursionlimit(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "ts = time.time()\n",
    "timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "dataPath = '/data/fs4/datasets/pcaps/smallFlows.pcap'\n",
    "#pcaps = rdpcap(dataPath)\n",
    "#sessionPrep = pcaps.sessions()\n",
    "#end = time.time()\n",
    "#print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_header(line):  # pragma: no cover\n",
    "    ret_dict = {}\n",
    "    h = line.split()\n",
    "    if h[2] == 'IP6':\n",
    "        \"\"\"\n",
    "        Conditional formatting based on ethernet type.\n",
    "        IPv4 format: 0.0.0.0.port\n",
    "        IPv6 format (one of many): 0:0:0:0:0:0.port\n",
    "        \"\"\"\n",
    "        ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "        ret_dict['src_ip'] = h[3].split('.')[0]\n",
    "        ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "        ret_dict['dest_ip'] = h[5].split('.')[0]\n",
    "    else:\n",
    "        if len(h[3].split('.')) > 4:\n",
    "            ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "            ret_dict['src_ip'] = '.'.join(h[3].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['src_ip'] = h[3]\n",
    "            ret_dict['src_port'] = ''\n",
    "        if len(h[5].split('.')) > 4:\n",
    "            ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "            ret_dict['dest_ip'] = '.'.join(h[5].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['dest_ip'] = h[5].split(':')[0]\n",
    "            ret_dict['dest_port'] = ''\n",
    "    return ret_dict\n",
    "\n",
    "\n",
    "def parse_data(line):  # pragma: no cover\n",
    "    ret_str = ''\n",
    "    h, d = line.split(':', 1)\n",
    "    ret_str = d.strip().replace(' ', '')\n",
    "    return ret_str\n",
    "\n",
    "\n",
    "def process_packet(output):  # pragma: no cover\n",
    "    # TODO!! throws away the first packet!\n",
    "    ret_header = {}\n",
    "    ret_dict = {}\n",
    "    ret_data = ''\n",
    "    hasHeader = False\n",
    "    for line in output:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if not line.startswith('0x'):\n",
    "                # header line\n",
    "                if ret_dict and ret_data:\n",
    "                    # about to start new header, finished with hex\n",
    "                    ret_dict['data'] = ret_data\n",
    "                    yield ret_dict\n",
    "                    ret_dict.clear()\n",
    "                    ret_header.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "\n",
    "                # parse next header\n",
    "                try:\n",
    "                    ret_header = parse_header(line)\n",
    "                    ret_dict.update(ret_header)\n",
    "                    hasHeader = True\n",
    "                except:\n",
    "                    ret_header.clear()\n",
    "                    ret_dict.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "\n",
    "            else:\n",
    "                # hex data line\n",
    "                if hasHeader:\n",
    "                    data = parse_data(line)\n",
    "                    ret_data = ret_data + data\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "def is_clean_packet(packet):  # pragma: no cover\n",
    "    \"\"\"\n",
    "    Returns whether or not the parsed packet is valid\n",
    "    or not. Checks that both the src and dest\n",
    "    ports are integers. Checks that src and dest IPs\n",
    "    are valid address formats. Checks that packet data\n",
    "    is hex. Returns True if all tests pass, False otherwise.\n",
    "    \"\"\"\n",
    "    if not packet['src_port'].isdigit(): return False\n",
    "    if not packet['dest_port'].isdigit(): return False\n",
    "\n",
    "    if packet['src_ip'].isalpha(): return False\n",
    "    if packet['dest_ip'].isalpha(): return False\n",
    "\n",
    "    if 'data' in packet:\n",
    "        try:\n",
    "            int(packet['data'], 16)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def order_keys(hexSessionDict):\n",
    "    \"\"\"\n",
    "    Returns list of the hex sessions in (rough) time order.\n",
    "    \"\"\"\n",
    "    orderedKeys = []\n",
    "\n",
    "    for key in sorted(hexSessionDict.keys(), key=lambda key: hexSessionDict[key][1]):\n",
    "        orderedKeys.append(key)\n",
    "\n",
    "    return orderedKeys\n",
    "\n",
    "\n",
    "def read_pcap(path):  # pragma: no cover\n",
    "    print 'starting reading pcap file'\n",
    "    hex_sessions = {} \n",
    "    proc = subprocess.Popen('tcpdump -nn -tttt -xx -r '+path,\n",
    "                            shell=True,\n",
    "                            stdout=subprocess.PIPE)\n",
    "    insert_num = 0  # keeps track of insertion order into dict\n",
    "    for packet in process_packet(proc.stdout):\n",
    "        if not is_clean_packet(packet):\n",
    "            continue\n",
    "        if 'data' in packet:\n",
    "            key = (packet['src_ip']+\":\"+packet['src_port'], packet['dest_ip']+\":\"+packet['dest_port'])\n",
    "            rev_key = (key[1], key[0])\n",
    "            if key in hex_sessions:\n",
    "                hex_sessions[key][0].append(packet['data'])\n",
    "            elif rev_key in hex_sessions:\n",
    "                hex_sessions[rev_key][0].append(packet['data'])\n",
    "            else:\n",
    "                hex_sessions[key] = ([packet['data']], insert_num)\n",
    "                insert_num += 1\n",
    "\n",
    "    print 'finished reading pcap file'\n",
    "    return hex_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pickleFile(thing2save, file2save2=None, filePath='/work/notebooks/drawModels/', fileName='myModels'):  # pragma: no cover\n",
    "\n",
    "    if file2save2 is None:\n",
    "        f = file(filePath+fileName+'.pickle', 'wb')\n",
    "    else:\n",
    "        f = file(filePath+file2save2, 'wb')\n",
    "\n",
    "    cPickle.dump(thing2save, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def loadFile(filePath):  # pragma: no cover\n",
    "    file2open = file(filePath, 'rb')\n",
    "    loadedFile = cPickle.load(file2open)\n",
    "    file2open.close()\n",
    "    return loadedFile\n",
    "\n",
    "\n",
    "def removeBadSessionizer(hexSessionDict, saveFile=False, dataPath=None, fileName=None):  # pragma: no cover\n",
    "    for ses in hexSessionDict.keys():\n",
    "        paclens = []\n",
    "        for pac in hexSessionDict[ses][0]:\n",
    "            paclens.append(len(pac))\n",
    "        if np.min(paclens)<80:\n",
    "            del hexSessionDict[ses]\n",
    "\n",
    "    if saveFile:\n",
    "        print 'pickling sessions'\n",
    "        pickleFile(hexSessionDict, filePath=dataPath, fileName=fileName)\n",
    "\n",
    "    return hexSessionDict\n",
    "\n",
    "\n",
    "# Making the hex dictionary\n",
    "def hexTokenizer():  # pragma: no cover\n",
    "    hexstring = '''0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F,\n",
    "                   10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 1A, 1B,\n",
    "                   1C, 1D, 1E, 1F, 20, 21, 22, 23, 24, 25, 26, 27,\n",
    "                   28, 29, 2A, 2B, 2C, 2D, 2E, 2F, 30, 31, 32, 33,\n",
    "                   34, 35, 36, 37, 38, 39, 3A, 3B, 3C, 3D, 3E, 3F,\n",
    "                   40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 4A, 4B,\n",
    "                   4C, 4D, 4E, 4F, 50, 51, 52, 53, 54, 55, 56, 57,\n",
    "                   58, 59, 5A, 5B, 5C, 5D, 5E, 5F, 60, 61, 62, 63,\n",
    "                   64, 65, 66, 67, 68, 69, 6A, 6B, 6C, 6D, 6E, 6F,\n",
    "                   70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 7A, 7B,\n",
    "                   7C, 7D, 7E, 7F, 80, 81, 82, 83, 84, 85, 86, 87,\n",
    "                   88, 89, 8A, 8B, 8C, 8D, 8E, 8F, 90, 91, 92, 93,\n",
    "                   94, 95, 96, 97, 98, 99, 9A, 9B, 9C, 9D, 9E, 9F,\n",
    "                   A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, AA, AB,\n",
    "                   AC, AD, AE, AF, B0, B1, B2, B3, B4, B5, B6, B7,\n",
    "                   B8, B9, BA, BB, BC, BD, BE, BF, C0, C1, C2, C3,\n",
    "                   C4, C5, C6, C7, C8, C9, CA, CB, CC, CD, CE, CF,\n",
    "                   D0, D1, D2, D3, D4, D5, D6, D7, D8, D9, DA, DB,\n",
    "                   DC, DD, DE, DF, E0, E1, E2, E3, E4, E5, E6, E7,\n",
    "                   E8, E9, EA, EB, EC, ED, EE, EF, F0, F1, F2, F3,\n",
    "                   F4, F5, F6, F7, F8, F9, FA, FB, FC, FD, FE, FF'''.replace('\\t', '')\n",
    "\n",
    "    hexList = [x.strip() for x in hexstring.lower().split(',')]\n",
    "    hexList.append('<EOP>')  # End Of Packet token\n",
    "    hexDict = {}\n",
    "\n",
    "    for key, val in enumerate(hexList):\n",
    "        if len(val) == 1:\n",
    "            val = '0'+val\n",
    "        hexDict[val] = key  #dictionary k=hex, v=int  \n",
    "\n",
    "    return hexDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hexSessions = read_pcap(dataPath)\n",
    "hexSessions = removeBadSessionizer(hexSessions)\n",
    "hexSessionsKeys = order_keys(hexSessions)\n",
    "hexDict = hexTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary of IP communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets=2, packetTimeSteps=100,\n",
    "                      packetReverse=False, charLevel=False, padOldTimeSteps=True):  # pragma: no cover\n",
    "    \n",
    "    sessionCollect = []\n",
    "    packetCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = copy(sessionPackets[:maxPackets])\n",
    "    else:\n",
    "        sessionList = copy(sessionPackets)\n",
    "\n",
    "    for packet in sessionList:\n",
    "        packet = packet[32:36]+packet[44:46]+packet[46:48]+packet[52:60]+packet[60:68]+\\\n",
    "                 packet[68:70]+packet[70:72]+packet[72:74]\n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "            \n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "            packet = packet[:packetTimeSteps]\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        else:\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "        packetCollect.append(packet)\n",
    "        \n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the\n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession,\n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    \n",
    "    return sessionCollect, packetCollect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    #if iteration%100 == 0:\n",
    "    #    updates.append((lr, lr*0.93))\n",
    "    #else:\n",
    "    #    updates.append((lr, lr))\n",
    "    \n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    #if iteration%100 == 0:\n",
    "    #    updates.append((lr, lr*0.93))\n",
    "    #else:\n",
    "    #    updates.append((lr, lr))\n",
    "    \n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for both the unsupervised net and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('inputs')\n",
    "Y = T.matrix('targets')\n",
    "\n",
    "wtstd = 0.2\n",
    "dimIn = 257 #hex has 256 characters + the <EOP> character\n",
    "dim = 100 #dimension reduction size\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = False\n",
    "linewt_init = IsotropicGaussian(wtstd)\n",
    "line_bias = Constant(1.0)\n",
    "rnnwt_init = IsotropicGaussian(wtstd)\n",
    "rnnbias_init = Constant(0.0)\n",
    "packetReverse = False\n",
    "\n",
    "###ENCODER\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "\n",
    "###CONTEXT\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "if bidirectional:\n",
    "    dimDec = dim*2\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        \n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "    \n",
    "    rnnContextRev.initialize()\n",
    "\n",
    "else:\n",
    "    dimDec = dim\n",
    "\n",
    "\n",
    "###DECODER\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dimIn, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dimIn, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkFinal', input_dim=dim, output_dims=[dimIn, dimIn*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "#initialize the weights in all the functions\n",
    "fork.initialize()\n",
    "rnn.initialize()\n",
    "\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()\n",
    "forkDec.initialize()\n",
    "\n",
    "forkFinal.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This section can be skipped if you want to go directly to the classifier\n",
    "\n",
    "learning_rate = theano.shared(np.array(0.001, dtype=theano.config.floatX))\n",
    "learning_decay = np.array(0.9, dtype=theano.config.floatX)\n",
    "batch_size = 20\n",
    "maxPackets = 3\n",
    "packetTimeSteps = 16\n",
    "attention = False\n",
    "\n",
    "def onestepEnc(X):\n",
    "        data1, data2 = fork.apply(X) \n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            hEnc = rnn.apply(data1, data2) \n",
    "        else:\n",
    "            hEnc, _ = rnn.apply(data2)\n",
    "\n",
    "        return hEnc, data1\n",
    "\n",
    "[hEnc, data1], _ = theano.scan(onestepEnc, X) #(mini*numPackets, packetLen, 1, hexdictLen)\n",
    "hEncReshape = T.reshape(hEnc[:,-1], (-1, maxPackets, 1, dim)) #[:,-1] takes the last rep for each packet\n",
    "                                                             #(mini, numPackets, 1, dimReduced)\n",
    "def onestepContext(hEncReshape):\n",
    "\n",
    "    data3, data4 = forkContext.apply(hEncReshape)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hContext, _ = rnnContext.apply(data4)\n",
    "\n",
    "    return hContext\n",
    "\n",
    "hContext, _ = theano.scan(onestepContext, hEncReshape)\n",
    "\n",
    "if attention:\n",
    "    print 'what are you doing?!?!'\n",
    "else:\n",
    "    data5, _ = forkDec.apply(hContext)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctxtest = theano.function([X], [data1, hContext], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testones = np.ones((12,16,1,257))\n",
    "testones[0] = testones[0]+3\n",
    "testones[-1] = testones[-1]-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctxtest(testones)[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(4, 2, 1, 100)\n",
    "ctxtest(testones)[1][:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#.reshape((4, maxPackets, packetTimeSteps, dim)) = (4, 3, 16, 100)\n",
    "#[:,1:,:-1] = (4, 2, 15, 100)\n",
    "ctxtest(testones)[0].reshape((4, maxPackets, packetTimeSteps, dim))[:,1:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data1, (batch_size, maxPackets, packetTimeSteps, dim))[:,1:,:-1]),\n",
    "#(12, 16, 1, 100)\n",
    "ctxtest(testones)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######START HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Transform hContext then add it to the concatenated zeros stuff\n",
    "\n",
    "#(4, 2, 16, 100)\n",
    "#                         (batch_size, maxPackets-1, 1, dim)\n",
    "(np.concatenate((np.zeros((4,2,1,100)),ctxtest(testones)[0].reshape((4, maxPackets, packetTimeSteps, dim))[:,1:,:-1]), axis = 2) + ctxtest(testones)[1][:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks.recurrent import BaseRecurrent, recurrent\n",
    "from blocks.bricks.recurrent import SimpleRecurrent\n",
    "from blocks import initialization\n",
    "from blocks.bricks import Identity\n",
    "#from blocks.initialization import Identity\n",
    "\n",
    "#@recurrent\n",
    "rnntest = SimpleRecurrent(\n",
    "          dim=6, activation=Identity(), name='second_recurrent_layer',\n",
    "          weights_init=initialization.Identity())\n",
    "\n",
    "rnntest.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdat1 = T.tensor3()\n",
    "testdat2 = T.matrix()\n",
    "testout = rnntest.apply(inputs = testdat1, states = testdat2, iterate = False)\n",
    "#testout = rnntest.apply(inputs = testdat1, states = testdat2, iterate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testy = theano.function([testdat1], testout, allow_input_downcast=True)\n",
    "testy = theano.function([testdat1, testdat2], testout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testx = np.random.rand(4,1,6)\n",
    "testx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testy(testx, np.ones((1,6), dtype=theano.config.floatX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testy(np.ones((3,1,6)), np.ones((3,1,6))+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testy(np.random.rand(4,1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #this fork makes data5 same dim as data1 (the orig word embedding)\n",
    "\n",
    "#decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "#and the last hidden state of the context RNN.\n",
    "#THINK about L2 pooling before cat\n",
    "#THINK should we concatenate with X instead of data5\n",
    "#if packetReverse:\n",
    "#    data1 = data1[:,::-1]\n",
    "#do we need data5??\n",
    "\n",
    "data7 = T.concatenate((data5[:,:-1], \n",
    "                       T.reshape(data1, (batch_size, maxPackets, packetTimeSteps, dim))[:,1:,:-1]), \n",
    "                      axis = 2)\n",
    "                      \n",
    "                      #data1 is the original embedding of X, data5 is transformed context output\n",
    "                      #get rid of first packet in data 5\n",
    "                      #get rid of last context vector\n",
    "\n",
    "#predicts all but the first packet, i.e. session[1:]\n",
    "#output.shape = (20, 2, 100, 257), (minibatch, \n",
    "def onestepDec(data7):\n",
    "    \n",
    "    data8, data9 = forkFinal.apply(data7) #forkFinal transforms back to original dimIn\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hDec = rnnDec.apply(data8, data9) \n",
    "    else:\n",
    "        hDec, _ = rnnDec.apply(data9)\n",
    "        #hDec = hinit #hDec shape = (batch_size*(maxPackets-1), packetTimeSteps, 257)\n",
    "    \n",
    "    return hDec\n",
    "\n",
    "\n",
    "hDec, _ = theano.scan(onestepDec, data7)\n",
    "\n",
    "hDecReshape = T.reshape(hDec, (batch_size*(maxPackets-1), packetTimeSteps, dimIn))\n",
    "\n",
    "\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(hDecReshape, extra_ndim = 1)\n",
    "predX = T.reshape(T.reshape(X,(batch_size, maxPackets, packetTimeSteps, dimIn))[:,1:,:,:], \n",
    "                  (batch_size*(maxPackets-1), packetTimeSteps, dimIn))\n",
    "\n",
    "precost = predX*T.log(softout) + (1-predX)*T.log(1-softout)\n",
    "precost2 = -T.sum(T.sum(precost, axis = 2), axis = 1)\n",
    "#precost2 = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "\n",
    "cost = T.mean(precost2)\n",
    "#cost = T.mean(BinaryCrossEntropy().apply(predX, softout))\n",
    "cg = ComputationGraph([cost])\n",
    "\n",
    "learning_rate = theano.shared(np.array(0.0001, dtype=theano.config.floatX))\n",
    "learning_decay = np.array(0.9, dtype=theano.config.floatX)\n",
    "\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "updates = Adam(params, cost, learning_rate, c=1) #c is gradient clipping parameter\n",
    "#updates = RMSprop(cost, params, learning_rate, c=1)\n",
    "\n",
    "#gradients = T.grad(cost, params)\n",
    "#gradients = clip_norms(gradients, 1)\n",
    "#gradientFun = theano.function([X], gradients, allow_input_downcast=True)\n",
    "\n",
    "print \"compiling you beautiful person\"\n",
    "train = theano.function([X], [cost, hContext], updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], [softout, hContext], allow_input_downcast=True)\n",
    "print \"finished compiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#randomize data\n",
    "hexSessionsKeys = hexSessions.keys()\n",
    "#random.shuffle(hexSessionsKeys)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "\n",
    "padOldTimeSteps = False\n",
    "\n",
    "runname = 'hred'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "contextCollect = []\n",
    "\n",
    "epochs = 80\n",
    "iteration = 0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "    \n",
    "    for start, end in zip(range(0, trainIndex,batch_size), range(batch_size, trainIndex, batch_size)):\n",
    "        \n",
    "        trainingSessions = []\n",
    "        \n",
    "        for trainKey in range(start, end):\n",
    "            sessionForEncoding = list(hexSessions[hexSessions.keys()[trainKey]])\n",
    "            \n",
    "            #encode a normal session\n",
    "            #oneHotSes = oneSessionEncoder(sessionForEncoding,hexDict = hexDict, packetReverse=packetReverse, \n",
    "            #                              padOldTimeSteps = padOldTimeSteps, maxPackets = maxPackets,\n",
    "            #                              packetTimeSteps = packetTimeSteps)\n",
    "            #trainingSessions.append(oneHotSes[0])\n",
    "            #trainingTargets.append(normalTarget)\n",
    "            \n",
    "            #encode an abby normal session\n",
    "            \n",
    "            oneHotSes = oneSessionEncoder(sessionForEncoding,\n",
    "                                              hexDict = hexDict,\n",
    "                                              packetReverse=packetReverse, \n",
    "                                              padOldTimeSteps = padOldTimeSteps, \n",
    "                                              maxPackets = maxPackets, \n",
    "                                              packetTimeSteps = packetTimeSteps)\n",
    "            \n",
    "            trainingSessions.append(oneHotSes[0])\n",
    "            \n",
    "        sessionsMinibatch = np.asarray(trainingSessions).reshape((batch_size*maxPackets, packetTimeSteps, 1, dimIn))        \n",
    "    \n",
    "        costfun = train(sessionsMinibatch)\n",
    "        costCollect.append(costfun[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if iteration == 0:\n",
    "            print 'you are amazing'\n",
    "            \n",
    "        iteration+=1\n",
    "        \n",
    "        #if iteration%80 == 0:\n",
    "        #    learning_rate.set_value(learning_rate.get_value() * learning_decay)\n",
    "        #    print '   learning rate: ', learning_rate.get_value()\n",
    "        \n",
    "    ####SAVE COST TO FILE \n",
    "    \n",
    "    if epoch%2 == 0:\n",
    "                \n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        contextCollect.append(costfun[1][:4])\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        #grads = gradientFun(inputs, outputs)\n",
    "        #for gra in grads:\n",
    "        #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    #np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Two huge extension:\n",
    "#    1) give encoder to all decoding time steps\n",
    "#    2) give predicted output to next time step as well\n",
    "#    CHECK 3) use bidirectional to use give beginning and end of encoder to decoder\n",
    "\n",
    "\n",
    "#294.692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "gpupredict = loadFile('/data/fs4/home/bradh/outputs/hredClassify8fullpacketsPREDICT52500.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gives list of parameters\n",
    "gpupredict.get_shared()[0].get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpupredict.get_shared()[7].get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadFile(filePath):  # pragma: no cover\n",
    "    file2open = file(filePath, 'rb')\n",
    "    loadedFile = cPickle.load(file2open)\n",
    "    file2open.close()\n",
    "    return loadedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graph of the model\n",
    "X = T.tensor4('inputs')\n",
    "Y = T.matrix('targets')\n",
    "rnnType = 'gru'\n",
    "\n",
    "#data specifics\n",
    "dimIn = 257\n",
    "maxPackets = 8\n",
    "dim = 100\n",
    "numClasses = 4\n",
    "\n",
    "###ENCODER\n",
    "if rnnType == 'gru':\n",
    "    #rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    rnn = GatedRecurrent(dim=dim)\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim)\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork = Fork(output_names=['linear', 'gates'], input_dim=dimIn, output_dims=[dim, dim * dimMultiplier])\n",
    "\n",
    "###CONTEXT\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim)\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim)\n",
    "\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'], input_dim=dim, output_dims=[dim, dim * dimMultiplier])\n",
    "\n",
    "forkDec = Fork(output_names=['linearOut', 'gatesOut'], input_dim=dim, output_dims=[dim, dim*dimMultiplier])\n",
    "\n",
    "#CLASSIFIER\n",
    "bmlp = BatchNormalizedMLP( activations=[Logistic(),Logistic()],dims=[dim, dim, numClasses])\n",
    "\n",
    "def onestepEnc(X):\n",
    "    data1, data2 = fork.apply(X) \n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc = rnn.apply(data1, data2) \n",
    "    else:\n",
    "        hEnc, _ = rnn.apply(data2)\n",
    "\n",
    "    return hEnc\n",
    "\n",
    "hEnc, _ = theano.scan(onestepEnc, X) #(mini*numPackets, packetLen, 1, hexdictLen)\n",
    "hEncReshape = T.reshape(hEnc[:,-1], (-1, maxPackets, 1, dim)) #[:,-1] takes the last rep for each packet\n",
    "                                                             #(mini, numPackets, 1, dimReduced)\n",
    "#hEncReshape = hEnc[:,-1].dimshuffle(0,1,'x',2)\n",
    "\n",
    "def onestepContext(hEncReshape):\n",
    "\n",
    "    data3, data4 = forkContext.apply(hEncReshape)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hContext, _ = rnnContext.apply(data4)\n",
    "\n",
    "    return hContext\n",
    "\n",
    "\n",
    "hContext, _ = theano.scan(onestepContext, hEncReshape)\n",
    "hContextReshape = T.reshape(hContext[:,-1], (-1,dim))\n",
    "\n",
    "data5, _ = forkDec.apply(hContextReshape)\n",
    "\n",
    "pyx = bmlp.apply(data5)\n",
    "softmax = Softmax()\n",
    "softoutClass = softmax.apply(pyx)\n",
    "\n",
    "classifierPredict = theano.function([X], softoutClass, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for wt in range(len(classifierPredict.get_shared())):\n",
    "    classifierPredict.get_shared()[wt].set_value(gpupredict.get_shared()[wt].get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24235097,  0.24206284,  0.2411546 ,  0.27443159],\n",
       "       [ 0.26621625,  0.24451931,  0.24428323,  0.24498121]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testrand = np.random.rand(16,80,1,257)\n",
    "classifierPredict(testrand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24235095,  0.24206282,  0.24115461,  0.27443156],\n",
       "       [ 0.26621613,  0.24451932,  0.24428324,  0.24498124]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpupredict(testrand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickleFile(classifierPredict, filePath='/data/fs4/home/bradh/outputs/', fileName='cpuversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
